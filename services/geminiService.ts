
import { GoogleGenAI, Modality, Part } from "@google/genai";

const API_KEY = process.env.API_KEY;

if (!API_KEY) {
    throw new Error("API_KEY environment variable not set");
}

const ai = new GoogleGenAI({ apiKey: API_KEY });

async function getBase64ImageFromResponse(response: any): Promise<string> {
    const candidate = response.candidates?.[0];

    const generatedPart = candidate?.content?.parts?.find(
        (part) => part.inlineData && part.inlineData.mimeType.startsWith('image/')
    );

    if (generatedPart?.inlineData?.data) {
        const mimeType = generatedPart.inlineData.mimeType;
        const base64Image = generatedPart.inlineData.data;
        return `data:${mimeType};base64,${base64Image}`;
    } else {
        console.error("No image part found in API response:", JSON.stringify(response, null, 2));
        const finishReason = candidate?.finishReason;
        if (finishReason === 'NO_IMAGE') {
             throw new Error("The AI model chose not to generate an image for this prompt. This can happen with complex requests. Please try a different prompt.");
        }
        if (finishReason === 'SAFETY') {
             throw new Error("The request was blocked by safety filters. Please modify your prompt or use a different image.");
        }
        throw new Error("No image was generated by the API.");
    }
}


export async function generateArchitecturalImage(imagePart: Part, textPrompt: string): Promise<string> {
    const textPart = { text: textPrompt };

    try {
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash-image',
            contents: {
                parts: [imagePart, textPart],
            },
            config: {
                responseModalities: [Modality.IMAGE],
            },
        });
        
        return getBase64ImageFromResponse(response);

    } catch (error) {
        console.error("Error generating image with Gemini API:", error);
        if (error instanceof Error) {
            try {
                // Attempt to parse a JSON error message from the API
                const parsedError = JSON.parse(error.message);
                if (parsedError.error && parsedError.error.message) {
                    throw new Error(`Failed to generate image: ${parsedError.error.message}`);
                }
            } catch (parseError) {
                // Not a JSON error, throw original message
                throw new Error(`Failed to generate image: ${error.message}`);
            }
        }
        throw new Error("Failed to generate image. Please check the console for more details.");
    }
}

export async function renderBuildingOnSite(buildingImagePart: Part, siteImagePart: Part): Promise<string> {
    const textPart = {
        text: `Photorealistically place the building from the first image into the location shown in the second image. The building should replace any existing structures in the center of the site image. Match the lighting, shadows, scale, and perspective to create a seamless, cohesive, and realistic architectural visualization. Ensure the final image looks like a professional render.`
    };
    
    try {
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash-image',
            contents: {
                parts: [buildingImagePart, siteImagePart, textPart],
            },
            config: {
                responseModalities: [Modality.IMAGE],
            },
        });

        return getBase64ImageFromResponse(response);

    } catch (error) {
        console.error("Error generating site render with Gemini API:", error);
        if (error instanceof Error) {
            throw new Error(`Failed to generate site render: ${error.message}`);
        }
        throw new Error("Failed to generate site render. Please check the console for more details.");
    }
}

// FIX: Add generateVideoFromImage function for video generation.
export async function generateVideoFromImage(
    imagePart: Part,
    prompt: string,
    aspectRatio: '16:9' | '9:16',
    setProgress: (message: string) => void
): Promise<string> {
    if (!(await (window as any).aistudio.hasSelectedApiKey())) {
        await (window as any).aistudio.openSelectKey();
    }
    
    // Create new instance to get latest key
    const aiWithApiKey = new GoogleGenAI({ apiKey: process.env.API_KEY });

    try {
        setProgress('Starting video generation...');
        let operation = await aiWithApiKey.models.generateVideos({
            model: 'veo-3.1-fast-generate-preview',
            prompt: prompt,
            image: {
                imageBytes: imagePart.inlineData!.data,
                mimeType: imagePart.inlineData!.mimeType,
            },
            config: {
                numberOfVideos: 1,
                resolution: '720p', // Keep it faster for the demo
                aspectRatio: aspectRatio
            }
        });
        
        setProgress('Processing video... this can take several minutes.');
        while (!operation.done) {
            await new Promise(resolve => setTimeout(resolve, 10000));
            operation = await aiWithApiKey.operations.getVideosOperation({ operation: operation });
            const progressPercentage = (operation.metadata as any)?.progressPercentage;
            if (progressPercentage) {
                setProgress(`Processing video... ${progressPercentage}% complete.`);
            }
        }

        if (operation.error) {
            throw new Error(`Video generation failed: ${operation.error.message}`);
        }

        const downloadLink = operation.response?.generatedVideos?.[0]?.video?.uri;
        if (!downloadLink) {
            throw new Error("Video generation completed, but no download link was found.");
        }
        
        setProgress('Fetching generated video...');
        const response = await fetch(`${downloadLink}&key=${process.env.API_KEY}`);
        if (!response.ok) {
            throw new Error(`Failed to download video: ${response.statusText}`);
        }
        const videoBlob = await response.blob();
        return URL.createObjectURL(videoBlob);

    } catch (error) {
        if (error instanceof Error && error.message.includes("Requested entity was not found.")) {
             await (window as any).aistudio.openSelectKey(); // Prompt user to re-select
             throw new Error("API key not found or invalid. Please select your API key and try again. A link to billing documentation can be found in the dialog: ai.google.dev/gemini-api/docs/billing");
        }
        console.error("Error generating video with Gemini API:", error);
        throw error;
    }
}

// FIX: Add generateDescriptionForNarration function for text generation.
export async function generateDescriptionForNarration(imagePart: Part): Promise<string> {
    const textPart = {
        text: "You are an architectural expert. Describe the building in this aerial-view image. Focus on its key design features, potential materials, and the overall style. Make your description engaging and concise, suitable for a short video narration."
    };
    
    try {
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash',
            contents: {
                parts: [imagePart, textPart]
            },
        });
        return response.text;
    } catch (error) {
        console.error("Error generating description with Gemini API:", error);
        if (error instanceof Error) {
            throw new Error(`Failed to generate description: ${error.message}`);
        }
        throw new Error("Failed to generate description. Please check the console for more details.");
    }
}

// FIX: Add generateSpeechFromText function for audio generation.
export async function generateSpeechFromText(text: string): Promise<string> {
    try {
        const response = await ai.models.generateContent({
            model: "gemini-2.5-flash-preview-tts",
            contents: [{ parts: [{ text: text }] }],
            config: {
                responseModalities: [Modality.AUDIO],
                speechConfig: {
                    voiceConfig: {
                        prebuiltVoiceConfig: { voiceName: 'Kore' }, // A standard, clear voice
                    },
                },
            },
        });

        const base64Audio = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
        if (!base64Audio) {
            throw new Error("API did not return audio data.");
        }
        return base64Audio;
    } catch (error) {
        console.error("Error generating speech with Gemini API:", error);
        if (error instanceof Error) {
            throw new Error(`Failed to generate speech: ${error.message}`);
        }
        throw new Error("Failed to generate speech. Please check the console for more details.");
    }
}

// FIX: Add analyzeBuilding function for text analysis.
export async function analyzeBuilding(imagePart: Part): Promise<string> {
    const textPart = {
        text: `Analyze the building in the provided aerial-view image from an architectural and urban planning perspective. Provide a detailed report in Markdown format. Cover the following aspects:
- **Architectural Style:** Identify the likely architectural style (e.g., Modern, Brutalist, Art Deco) and justify your reasoning.
- **Form and Massing:** Describe the building's overall shape, volume, and how its different parts are composed.
- **Facade and Materials:** Analyze the building's exterior, speculating on potential materials (e.g., concrete, glass, steel) and describing any notable features like windows, patterns, or textures.
- **Site Context and Urban Integration:** Evaluate how the building relates to its surroundings. Consider its scale, relationship to the street, landscaping, and potential impact on the urban environment.
- **Potential Use:** Speculate on the building's likely function (e.g., office, residential, cultural) based on its design.`
    };

    try {
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-pro', // Using pro for a more detailed analysis
            contents: {
                parts: [imagePart, textPart]
            },
        });
        return response.text;
    } catch (error) {
        console.error("Error analyzing building with Gemini API:", error);
        if (error instanceof Error) {
            throw new Error(`Failed to analyze building: ${error.message}`);
        }
        throw new Error("Failed to analyze building. Please check the console for more details.");
    }
}

// FIX: Add generateMultiViewNarrationScript function for script generation.
export async function generateMultiViewNarrationScript(imagePart: Part): Promise<string> {
    const textPart = {
        text: `You are a scriptwriter for architectural documentaries. Write a short, engaging narration script for a video showcasing the building in the provided image. The video will present four distinct views in this order: an aerial shot, a street-level side profile, a dramatic low-angle "ant's eye" view, and finally, a glimpse of the main interior space.

Your script should be around 100-150 words. It needs to flow smoothly from one perspective to the next, creating a compelling story about the building's design and character. Start with a broad overview from the aerial shot and gradually focus on more specific details, ending with the experience of being inside.`
    };

    try {
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash',
            contents: {
                parts: [imagePart, textPart]
            },
        });
        return response.text;
    } catch (error) {
        console.error("Error generating narration script with Gemini API:", error);
        if (error instanceof Error) {
            throw new Error(`Failed to generate narration script: ${error.message}`);
        }
        throw new Error("Failed to generate narration script. Please check the console for more details.");
    }
}
